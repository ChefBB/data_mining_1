\chapter{Clustering}
\label{ch:capitolo2}

This chapter aims at illustrating the clustering analysis performed on the dataset at hand.
The employed clustering techniques are K-means (Centroid-based), DBSCAN (density-based) and hierarchical clustering.
The analysis is conducted using these methods on a selection of the continuous attributes of the dataset, 
which were appropriately log-transformed (when needed - as mentioned in subsection 1.3.2), and normalized using \texttt{MinMaxScaler}. 

% For the K-means algorithm, \texttt{totalNominations} and \texttt{totalMedia} were excluded due to their high proportion of zero values, which negatively affected cluster formation.
% In addition, an attempt was made to incorporate categorical variables to the analysis with the K-means algorithm by converting them into binary attributes and constructing a mixed-distances matrix. 
% Distances were then calculated using the Euclidean distance for numerical (log-tranformed and scaled) features and the Jaccard similarity for binary ones. 
% However, this approach was computationally expensive and did not lead to any improvement in the results.


\section{K-means}\label{sec:centroid_based}
%The clustering analysis performed with the K-means algorithm focused on the numeric variables of the dataset, excluding \texttt{awardWins}, \texttt{awardNominationsExcludeWins}, and \texttt{totalCredits} due to their high proportion of zero values, 
% which negatively affected cluster formation. 
%The variables have been appropriately log-transformed (as illustrated in the \textit{Variable Transformation} section) and normalized with \texttt{StandardScaler}.

Clustering analysis with K-means was performed using a carefully selected subset of features. 
This step was motivated by the algorithm's sensitivity to the curse of dimensionality, as including too many variables can negatively affect SSE and Silhouette scores.
For this reason, although only \texttt{numVotes}, \texttt{totalCredits}, \texttt{userReviewsTotal}, \texttt{runtimeMinutes}, and \texttt{criticReviewsTotal},
were chosen for this task, they proved to be a solid choice due to their ability to represent meaningful aspects of the data.
To identify the proper number of clusters, both SSE and Silhouette scores were evaluated. 
The objective was to find a configuration that reduces the SSE while maintaining a robust Silhouette score and a proper \textit{k}. \\

The plots in Figure~\ref{fig:sse_silh_kmeans} show that \textit{k} = 4 provides a balance between the two values (obtaining SSE equal to 535.51 and 0.315 for the Silhouette score).
Only for visualization purposes, Principal Component Analysis (PCA) was employed. 
The first two components account for 58.01\% and 21.48\% of the total variance, indicating that this projection provides a fairly informative view of the clustering structure. 
The resulting clusters are presented in figure~\ref{fig:kmeans_visualization}. 
The 4 distinct clusters appear well-separated, suggesting that K-means managed to capture meaningful groupings in the data.



% To identify the optimal number of clusters, both the SSE and Silhouette scores were computed. The goal was to find a configuration that minimizes the SSE while maintaining a robust Silhouette score and a proper \textit{k}. 
% The plots in figure~\ref{fig:sse_silh_kmeans} demonstrate that \textit{k} = ??? provides the optimal balance between these metrics. Choosing \textit{k} = ??? returns a SSE score of ??? and Silhouette score of ???. 
% \textbf{VALORI TROPPO ALTI, VEDERE SE CAMBIANO CAMBIANDO SCALING E/O TOGLIENDO OUTLIER}

% To visualize the clustering results, Principal Component Analysis (PCA) was employed. The plot in figure~\ref{fig:pca_kmeans} reveals that 4 principal components are enough to capture the optimal amount of variance for the selected variables, as evidenced by the the point where the line starts to flatten, indicating that adding more components doesn't increase explained variance significantly.
 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.42\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/sse_silh_kmeans_def.png}
        \caption{SSE and Silhouette scores}
        \label{fig:sse_silh_kmeans}
    \end{subfigure}
    \begin{subfigure}[b]{0.42\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/kmeans_clusters.png}
        \caption{Clustering Visualization with PCA}
        \label{fig:kmeans_visualization}
    \end{subfigure}
    \caption{K-means - Cluster analysis}
    \label{fig:subplots_kmeans}
\end{figure}
% The distribution of data points across the four clusters is as follows (shown in percentage of data points per cluster):
% Red (0): 51.68\%, Blue (1): 7.42\%, Green (2): 24.22\%, Orange (3): 16.68\%.
% The clusters are not as well-separated in most Principal Component combinations as they are with PC1. 
% In fact, in the other combinations, the clusters tend to overlap and their boundaries are not always clearly distinct.
% This might be an indication that the true clusters have irregular shapes or different densities, resulting in boundaries between them being not clearly defined.



\section{DBSCAN}\label{sec:density_based}
To determine suitable DBSCAN parameters, the k-th distance plot was used as starting point (Figure~\ref{fig:dbscan_kth}). 
By varying \textit{k} (\textit{MinPts}) and observing the corresponding distance curves, the range between 0.08 and 0.2 was identified for a possible \textit{Eps} value.
The algorithm was applied to the following features: \texttt{numVotes}, \texttt{totalCredits}, \texttt{criticReviewsTotal}, \texttt{userReviewsTotal}, and \texttt{runtimeMinutes}. 
Although different combinations were tested, they all led to similar results. 
Effectively, initial experiments clearly revealed DBSCAN's sensitivity to highly sparse and imbalanced data; 
in most cases, the algorithm produced a highly populated single cluster corresponding to the dominant dense region, 
while leaving the sparser regions largely unstructured.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/dbscan_kth.png}
        \caption{K-th distance plot}
        \label{fig:dbscan_kth}
    \end{subfigure}
    \begin{subfigure}[b]{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/dbscan_scatterplot.png}
        \caption{Visualization with PCA}
        \label{fig:dbscan_visualization}
    \end{subfigure}
    \caption{DBSCAN - Cluster analysis}
    \label{fig:subplots_DBSCAN}
\end{figure}
To address this issue and obtain more meaningful clustering results, different combinations of \texttt{min\_eps} and \texttt{min\_points} in the following ranges were tested:
\begin{itemize}
    \item \texttt{eps\_list} =  [0.08, 0.09, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.3, 0.4, 1.0];
    \item \texttt{min\_points\_list} =  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20].
\end{itemize}
Each configuration was evaluated based on the number of clusters (with a minimum threshold of 2), their size, noise points, and Silhouette score 
(filtered for values $>$ 0.1), excluding cases with a single cluster or very low Silhouette values.
Although results were not fully satisfactory, the most relevant configurations are summarized in Table~\ref{tab:eps}. 
The reported Silhouette scores are computed excluding noise points; therefore, no score is given when only one cluster is formed. 
Note that, in this context, high Silhouette scores can be misleading, as they can favor compactness over structural granularity,
\begin{table}[h]
% \hspace*{-0.5cm}
\centering
\begin{tabular}{llcl}
\toprule
\textbf{eps threshold} & \textbf{Min points} & \textbf{Silhouette} & \textbf{Cluster composition}  \\
\midrule
\texttt{\textbf{$\varepsilon$} $>$ $0.16$} & for any tested value & \texttt{$-$} & never led to more than one cluster \\
\texttt{$0.13$ $\leq$ \textbf{$\varepsilon$} $\leq$ $0.16$} & for any tested value &  $[0.54,\ 0.70]$  & led to one cluster and, occasionally, 1 or 2 minor ones\\
\texttt{\textbf{$\varepsilon$} $<$ $0.13$}& for any tested value & $[0.10,\ 0.31]$ & the big cluster it's splitted in two different ones \\
\texttt{\textbf{$\varepsilon$} $\leq$ $0.10$} & lower values $[4,\ 5]$& $[0.10,\ 0.25]$  & led to several, not meaningful, clusters of 5/6 points\\
\bottomrule
\end{tabular}
\caption{Relevant DBSCAN parameters configurations}
\label{tab:eps}
\end{table}

Excluding the combination with higher \textit{Eps} value (that made the points collapse into one cluster), and the ones with low \textit{Eps} and few \texttt{min\_pts} (that resulted in over-fragmentation)
the analysis focused on \textit{Eps} in the range $[0.11,\ 0.13)$.
Despite the limitations of the dataset structure, in this range the algorithm was able to differentiate between regions of varying density.
% producing several clusters of very few elements (5/6), typically corresponding to isolated outliers or noise points rather than meaningful groups, 

One of the most interesting compromise (though still suboptimal in absolute terms) was found with \textit{Eps} = 0.11, \texttt{min\_points} = 7: 
As shown in (Figure~\ref{fig:dbscan_visualization}), this setting resulted in a three-cluster structure. A compact one of 10378 points was extracted from the densest region.
A second less dense cluster with fewer points (4347) captured intermediate-density areas or mild outliers. 
Finally, a very small but well-separated cluster, of only 11 points, effectively captured a potential relevant local pattern. 
In addition, 219 points were labeled as noise.
The resulting Silhouette Score was 0.2699.\\

% \textbf{QUESTA PARTE VOLENDO PUÃ² ANDARE NELLE CONSIDERAZIONI GENERALI}\\
% To conclude, these observations highlight DBSCAN's limitations when applied to datasets with skewed feature distributions and unbalanced local densities.
% Nonetheless, the exploration helped isolate parameter ranges where meaningful subclusters begin to emerge,beyond the dominant density mass.







% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{plots/DBSCAN_kth_graph.png}
%         \caption{k\textsuperscript{th} nearest neighbors}
%         \label{fig:DBSCAN_kth_graph}
%     \end{subfigure}
%     % \hfill
%     % \begin{subfigure}[b]{0.3\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{plots/pca_kmeans.png}
%     %     \caption{PCA Analysis}
%     %     \label{fig:pca_kmeans}
%     % \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{plots/DBSCAN_provvisoria.png}
%         \caption{DBSCAN Visualization}
%         \label{fig:DBSCAN_provvisoria}
%     \end{subfigure}
%     \caption{DBSCAN clustering analysis}
%     \label{fig:three_subplots}
% \end{figure}




\section{Hierarchical clustering}\label{sec:hierarchical}
Hierarchical clustering was performed using all linkages (Ward, Average, Complete, Single), with the Euclidean
distance metric. After a careful analysis of multiple clusterings, it was decided to procede with
all remaining numerical features.
Figure~\ref{fig:hier_clust_stats} shows the results of the analysis, which includes the Silhouette and SSE
scores, as well as the maximum and minimum percentage of points per cluster.\\

From figure~\ref{fig:max_min_pctg}, it can be observed that Single linkage produces a single cluster which contains basically all data points. This makes it unsuitable for this use case.
Average and Complete linkages produce a cluster with a high maximum cluster size (above 90\% of the dataset for most
of the number of clusters tested for Average, and 80\% for Complete).
These results are likely due to two main causes: 
\begin{itemize}
    \item Usage of skewed features, which lead to areas with a very high density of points;
    \item High dimensionality of the dataset, which makes it more difficult to separate clusters effectively.
\end{itemize}

These issues are mitigated by Ward's method, which doesn't show a dominant biggest cluster, for
all numbers of clusters tested. The minimum cluster size is also consistently more balanced
across different numbers of clusters.\\
\begin{figure}[H]
    \centering
    % Left: Tall figure (a)
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{plots/max_min_pctg.png}
        \subcaption{Max/Min percentage of points per cluster}
        \label{fig:max_min_pctg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{plots/sil_sse_hierarchical_clust.png}
        \subcaption{Silhouette and SSE scores}
        \label{fig:sil_sse_hierarchical_clust}
    \end{subfigure}
    \caption{Hierarchical clustering metrics for different numbers of clusters}
    \label{fig:hier_clust_stats}
\end{figure}


Since Ward's method is based on Squared Error, it's not surprising how its SSE is consistently lower than other linkages, as shown in the second graph of figure~\ref{fig:sil_sse_hierarchical_clust}.
More interesting is that Ward's method has the lowest Silhouette scores for all numbers
of clusters tested, which indicates that the clusters are not well separated. This is due to
the fact that Ward's method groups the data which resides in the high-density area
mentioned above into smaller clusters, leading to less well-defined boundaries between them.
In the next two sections, the results of Ward's method and Complete Linkage will be analyzed;
the other two will not be discussed further, as they provide limited insights on the dataset.

% By looking at Ward's statistics, the more interesting numbers of clusters to analyze are 4 and 5.
% The first one corresponds to a big decline in maximum cluster size (from 74 to 44\%),
% with a minimum cluster size of around 11\%; the second one has the same maximum cluster size but a smaller minimum cluster size (around 6\%), but has slightly higher Silhouette score
% (17.1\% against 16.8\% for 4 clusters) and a lower SSE. Figure~\ref{fig:dendrograms_ward} shows
% the dendrograms for the two clusterings.
\subsection{Ward's method}

% Both clusterings are set to have 4 clusters, which in both cases leads to cleaner dendrograms.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/dendrogram_4.png}
        \caption{Ward's Method Dendrogram}
        \label{fig:dendrogram_ward}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/scatter_ward.png}
        \caption{Ward's Method Scatter plot}
        \label{fig:scatter_ward}
    \end{subfigure}
    \caption{Ward's method clustering}
    \label{fig:dendrograms_ward}
\end{figure}
Figure~\ref{fig:dendrograms_ward} shows a dendrogram and a scatter plot of the clustering obtained through
Ward's Method. The cut at 4 clusters was chosen based on the parameters discussed in the previous section
and on the clarity of the dendrogram.\\

The dendrogram in Figure~\ref{fig:dendrogram_ward} reveals well-separated clusters, all of which merge at a
similar linkage distance (approximately 12). This indicates that the increase in within-cluster sum of
squares (SSE) is relatively consistent across all cluster merges, as expected from Ward's method.
Notably, the smallest cluster is the last to be merged, which suggests it is more distinct from the others.
As shown in Figure~\ref{fig:scatter_ward}, this cluster lies in a sparser region of the dataset.
In contrast, the remaining three clusters originate from the denser region, and are therefore spatially
closer to one another.
Compared to K-Means, the clusters identified by Ward's method appear less clearly separated in the PCA
projection, resulting in some overlap between adjacent groups.

% The obtained dendrogram has clearly separated clusters, as shown in figure~\ref{fig:dendrogram_ward}.
% The clusters all form at a similar distance (around 12), meaning the SSE increase is similar for
% all clusters.
% It is interesting to note how the smallest cluster is merged with the others at the last merge; this,
% as can be observed in figure~\ref{fig:scatter_ward}, is the cluster which resides in the less dense area of the dataset.
% The other three clusters are closer to each other, as they are a result of the split of the higher density
% area of the dataset. Compared to K-Means, the clusters are not as cleanly separated in the PCA space,
% leading to overlapping areas between them.




\subsection{Complete Linkage}
Figure~\ref{fig:dendrograms_complete} shows the dendrogram and scatter plot for a clustering obtained
through Complete Linkage.
Since the tendency of this linkage is to merge all points into a single cluster, the clustering is cut at 4 clusters,
which helps mitigating the issue. While selecting five clusters would have introduced an additional split
within the largest cluster with similar SSE and Silhouette scores, the resulting group was found to be
poorly separated and lacked meaningful distinction.
As such, it was not considered a valuable contribution to the overall clustering structure.\\

As shown in the dendrogram in Figure~\ref{fig:dendrogram_complete}, the clusters merge at similar linkage
distances (approximately between 1.2 and 1.4), indicating that the maximum within-cluster distances are
comparable across clusters. With respect to the clustering obtained through Ward's method, the clusters
have clearer boundaries along the PCA axes, as the denser area of the dataset is not split into multiple
clusters.\\

It is also interesting to observe how the dendrogram structure differs from that of Ward's method.
In the previous case, the smallest cluster was the last to be merged, reflecting its distinctiveness in
terms of within-cluster variance. In contrast, the Complete Linkage dendrogram shows the two smaller
clusters being merged before the root. This is observed with Single and Average Linkages as well, and
is a product of the sparsity of these clusters' regions.
% proximity in terms of maximum inter-point distance makes them more similar to each other than to the
% remaining data.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/dendrogram_complete.png}
        \caption{Complete Linkage Dendrogram}
        \label{fig:dendrogram_complete}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/scatter_complete.png}
        \caption{Complete Linkage Scatter plot}
        \label{fig:pairplot_ward_5}
    \end{subfigure}
    \caption{Dendrograms for hierarchical clustering with Complete Linkage}
    \label{fig:dendrograms_complete}
\end{figure}

\section{General considerations}\label{sec:considerations}
K-means achieved a high Silhouette score (0.315) and low SSE (535.51), but only after reducing the number of variables. 
This reflects the algorithm's tendency to perform well in simplified feature spaces, though at the cost of losing some informational richness.\\
In comparison, hierarchical clustering using Ward's method, which also minimizes SSE, 
led to lower performance (Silhouette 0.140, SSE 1329), but more variables were kept,
providing a broader view of the dataset. The resulting clusters were less clearly
separated, but Ward's method is better at capturing hierarchical structures, which can be informative in exploratory analysis. In addition, given that the dataset is sparse and unbalanced, a high 
Silhouette score alone is not a reliable indicator of clustering quality.\\
DBSCAN produced acceptable Silhouette scores as well (0.269, excluding noise), 
but this is largely due to its tendency to merge dense areas together, while treating isolated points as noise. 
While this can improve internal cohesion metrics, it does not reflect meaningful segmentation, especially when the data contains a dominant dense area. 
However, the analysis revealed parameter ranges where meaningful subclusters start to emerge, beyond the dominant density mass.\\
Overall, each algorithm showed trade-offs: K-means achieved compact clusters with fewer variables, 
hierarchical clustering with Ward's method retained more information but less separation, 
and DBSCAN captured dense areas but was sensitive to data structure and tuning.


