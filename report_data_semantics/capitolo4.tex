\chapter{Regression}
\label{ch:capitolo4}
Different regression techniques were applied to the dataset, testing
on different combinations of attributes. The target variable chosen for
Univariate and Multiple regression was \texttt{criticReviewsTotal}, while
for Multivariate regression, the target variables were both
\texttt{userReviewsTotal} and \texttt{criticReviewsTotal}.
These were chosen because they offer important insights into the engagement
that a product can generate, which also was the focus of the binary
classification task in section~\ref{sec:binary_classification}.


\section{Univariate and Multiple Regression}
For Univariate Regression, the attribute
\texttt{criticReviewsTotal} was chosen
as the target variable. Aside from the semantic meaning, this choice was also made
because it has a high correlation
with the attribute \texttt{userReviewsTotal}, allowing univariate regression
to be performed, while maintaining a clear separate semantic meaning.
Multivariate Regression was performed with \texttt{numVotes}, \texttt{numRegions} and
\texttt{totalMedia} as additional features, because of their high correlations with the target variable.
All features were normalized using StandardScaler, and all models' parameters
were optimized using Cross-Validation.
Table~\ref{tab:uni_multi_regression_report} shows the test performances of the different regressors.
\begin{table}[H]
    \centering
    \begin{tabular}{lccccccc}
        \toprule
        % mae, mse normalized over target variable range
         & \textbf{Intercept} & \textbf{Coefficient} & \textbf{R$^2$} & \textbf{MAE} & \textbf{MSE} & $\alpha$ \\
        \midrule
        \textbf{Univariate} \\
        \midrule
        Linear & -1.16* 10$^{-16}$ & 0.732 & 0.565 & 0.432 & 0.474 & - \\ % train mae: 0.6427363729986589
        Ridge & -1.16* 10$^{-16}$ & 0.732 & 0.565 & 0.432 & 0.474 & 1* 10$^{-4}$ \\ % alpha=100
        Lasso & -1.16* 10$^{-16}$ & 0.732 & 0.565 & 0.432 & 0.474 & 1* 10$^{-4}$ \\ % alpha=100
        DT & - & - & 0.601 & 0.446 & 0.434 & - \\
        % {'ccp_alpha': 0.004077514155476392, 'criterion': 'friedman_mse', 'max_depth': 35,
        % 'max_leaf_nodes': 19, 'min_impurity_decrease': 0.07111495324380178, 'min_samples_leaf': 19,
        % 'min_samples_split': 8}
        47-NN & - & - & 0.603 & 0.424 & 0.432 & - \\
        \midrule
        \textbf{Multiple}\\
        \midrule
        Linear & - & - & 0.628 & 0.436 & 0.405 & - \\
        Ridge & - & - & 0.628 & 0.436 & 0.405 & 37.649 \\ % alpha=0.1
        Lasso & - & - & 0.608 & 0.438 & 0.427 & 0.1 \\ % alpha=0.1
        DT & - & - & 0.659 & 0.395 & 0.371 & - \\
        % (ccp_alpha=0.0006952130531190704, criterion='friedman_mse',
        % max_depth=8, min_samples_leaf=9, min_samples_split=7)
        15-NN & - & - & 0.667 & 0.383 & 0.362 & - \\
        \bottomrule
    \end{tabular}
    \caption{Classification report for binary classification}
    \label{tab:uni_multi_regression_report}
\end{table}
In Univariate Regression, the Linear, Ridge, and Lasso models underperformed compared to both Decision
Tree and K-Nearest Neighbors regressors. Neither L1 nor L2 regularization significantly improved
performance, suggesting that the underlying relationship between the variables may not be well
captured by a linear model. 
Interestingly, the Decision Tree's Mean Absolute Error was the highest, but the Mean Squared Error was
lower than the linear models, indicating that its predictions were more accurate
on average, but with some outliers that were not well predicted.
K-Nearest Neighbors was the best performer, achieving better scores in all metrics.

In Multiple Regression, all models were able to express more of the variance in the target variable,
as can be seen from the R-Squared values' rise.
Interestingly, all Linear models had a slight drop in Mean Absolute Error performance compared to the
Univariate Regression, indicating an average error growth. However, their Mean Squared Error decreased,
suggesting that the models made fewer severe mistakes or large errors.
The Ridge model had the same performance as the Linear model, but with a regularization
parameter of 37.649, which is significantly higher than the one used in Univariate Regression.
The Lasso model, on the other hand, had a lower regularization parameter of 0.1, which is
still higher than the one used in Univariate Regression, but its performance was worse than the other
linear models.
The Decision Tree and K-Nearest Neighbors regressors performed better than the linear models,
with the K-Nearest Neighbors achieving the best performance in all metrics.

An interesting observation is that both Decision Tree and K-Nearest Neighbors regressors achieved their
best performances with higher tree depth and larger numbers of neighbors, respectively, in the
Univariate Regression. In contrast, in Multiple Regression, they performed better with lower tree depth
and fewer neighbors. Specifically, the Decision Tree depths were 35 (Univariate) and 8 (Multiple),
while the K-Nearest Neighbors' numbers of neighbors were 47 and 15. This suggests that in the
Univariate setting, the models needed greater complexity to capture the potentially nonlinear
relationship between the single feature and the target. Meanwhile, in Multiple Regression, the
combined features likely captured the relationship more linearly or simply, allowing the models to
generalize better with reduced complexity.



\section{Multivariate Regression}
