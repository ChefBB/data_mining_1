\chapter{Regression}
\label{ch:capitolo4}
Different regression techniques were applied to the dataset, testing
on different combinations of attributes. The target variable chosen for
Univariate and Multiple regression was \texttt{criticReviewsTotal}, while
for Multivariate regression, the target variables were both
\texttt{userReviewsTotal} and \texttt{criticReviewsTotal}.
These were chosen because they offer important insights into the engagement
that a product can generate, which also was the focus of the binary
classification task in section~\ref{sec:binary_classification}.


\section{Univariate and Multiple Regression}
For Univariate Regression, the attribute
\texttt{criticReviewsTotal} was chosen
as the target variable. Aside from the semantic meaning, this choice was also made
because it has a high correlation
with the attribute \texttt{userReviewsTotal}, allowing univariate regression
to be performed, while maintaining a clear separate semantic meaning.
Multivariate Regression was performed with \texttt{numVotes}, \texttt{numRegions} and
\texttt{totalMedia} as additional features, because of their high correlations with the target variable.
All models' parameters
were optimized using Cross-Validation, using Negative Mean Squared Error as scoring criterion.
Table~\ref{tab:side_by_side_regression} shows the test performances of the different regressors.
% \begin{table}[H]
%     \centering
%     \begin{tabular}{lc@{\hskip 30pt}ccc}
%         \toprule
%         % mae, mse normalized over target variable range
%          & \textbf{R$^2$} & \textbf{MAE} & \textbf{MSE} \\
%         \midrule
%         \multicolumn{4}{c}{\textbf{Univariate}} \\
%         \midrule
%         Linear & 0.465 & 3.419 & 189.543 \\
%         Ridge & 0.465 & 3.419 & 189.550 \\
%         Lasso & 0.279 & 3.948 & 255.732 \\
%         DT & 0.700 & 2.313 & 106.531 \\ % max_depth=35
%         47-NN & 0.643 & 2.221 & 126.539 \\ % k=24
%         \midrule
%         \multicolumn{4}{c}{\textbf{Multiple}} \\
%         \midrule
%         Linear & 0.628 & 3.125 & 132.002 \\
%         Ridge & 0.627 & 3.071 & 132.119 \\ % alpha=1000
%         Lasso & 0.610 & 2.731 & 138.249 \\ % alpha=10
%         DT & 0.702 & 2.119 & 105.854 \\ % max depth=8
%         15-NN & 0.673 & 2.268 & 105.864 \\ % 15 neighbors
%         \bottomrule
%     \end{tabular}
%     \caption{Performance report for Univariate and Multiple Regression}
%     \label{tab:uni_multi_regression_report}
% \end{table}

\begin{table}[H]
\centering
\makebox[\textwidth][c]{ % Centro forzato su tutta la larghezza
\begin{minipage}{0.44\textwidth}
    \centering
    \begin{tabular}{lc@{\hskip 5pt}cc}
        \toprule
         & \textbf{R$^2$} & \textbf{MAE} & \textbf{MSE} \\
        \midrule
        \multicolumn{4}{c}{\textbf{Univariate}} \\
        \midrule
        Linear & 0.465 & 3.419 & 189.543 \\
        Ridge & 0.465 & 3.419 & 189.550 \\
        Lasso & 0.279 & 3.948 & 255.732 \\
        DT & 0.700 & 2.313 & 106.531 \\
        47-NN & 0.643 & 2.221 & 126.539 \\
        \bottomrule
    \end{tabular}
    \caption*{(a) Univariate Regression}
\end{minipage}
\hspace{1pt} % distanza quasi nulla
\begin{minipage}{0.44\textwidth}
    \centering
    \begin{tabular}{lc@{\hskip 5pt}cc}
        \toprule
         & \textbf{R$^2$} & \textbf{MAE} & \textbf{MSE} \\
        \midrule
        \multicolumn{4}{c}{\textbf{Multiple}} \\
        \midrule
        Linear & 0.628 & 3.125 & 132.002 \\
        Ridge & 0.627 & 3.071 & 132.119 \\
        Lasso & 0.610 & 2.731 & 138.249 \\
        DT & 0.702 & 2.119 & 105.854 \\
        15-NN & 0.673 & 2.268 & 105.864 \\
        \bottomrule
    \end{tabular}
    \caption*{(b) Multiple Regression}
\end{minipage}
}
\caption{Performance comparison between Univariate and Multiple Regression}
\label{tab:side_by_side_regression}
\end{table}


The linear models underperformed compared to both Decision
Tree and K-Nearest Neighbors regressors in both tasks. In the first regression, neither L1 nor L2
regularization improved performances, with Lasso performing worse than the other linear models.
The same goes for Multiple Regression, but in this case L1 regularization had a slightly lower Mean Absolute Error
than the other linear models.
All linear models had a significant boost in all performance metrics with the additional features in Multiple Regression.
Here, although performances were still worse than the other two, Lasso regression had a lower Mean Absolute Error.

K-NN regressors had solid performances, with a slight increase in Multiple Regression for the Mean Absolute Error,
but a significantly lower Mean Squared Error.
The best performing model in both tasks was the Decision Tree regressor, although the additional features
offered a slight improvement in performance. An interesting observation is that the Cross-Validation
best hyperparameters for Univariate Regression indicated a high maximum depth of 35,
while the best hyperparameter for Multiple Regression was a maximum depth of 8; a similar trend was observed
in the K-NN regressors, where the optimal number of neighbors was 47 for Univariate Regression and 15 for Multiple Regression.
This suggests that the additional features in Multiple Regression allowed the model to generalize better,
reducing the need for a more complex model.



\section{Multivariate Regression}
As for Univariate and Multiple regressions, independent variables were
chosen by studying the correlations of other features with the target variables.
The chosen target variables (\texttt{userReviewsTotal} and \texttt{criticReviewsTotal})
shared the same top correlated features, which were found to be \texttt{totalCredits},
\texttt{numRegions} and \texttt{totalMedia}.
As in previous regression tasks, hyperparameters were optimized using Cross-Validation,
using Negative Mean Squared Error as scoring criterion .
Table~\ref{tab:multi_regression_report} shows the test performances of the different regressors.
\begin{table}[H]
    \centering
    \begin{tabular}{lccc@{\hskip 30pt}ccc}
        \toprule
        % mae, mse normalized over target variable range
         & \textbf{user R$^2$} & \textbf{user MAE} & \textbf{user MSE} & \textbf{critic R$^2$} & \textbf{critic MAE} & \textbf{critic MAE} \\
        \midrule
        Linear & 0.812 & 5.877 & 721.100 & 0.623 & 3.339 & 131.068 \\
        Ridge & 0.812 & 5.877 & 721.099 & 0.623 & 3.339 & 131.069 \\ % alpha=0.1
        Lasso & 0.812 & 5.624 & 722.818 & 0.604 & 3.113 & 137.847 \\ % alpha=1
        DT & 0.781 & 4.100 & 841.425 & 0.706 & 2.247 & 102.348 \\ % max_depth=45
        22-NN & 0.761 & 4.268 & 917.439 & 0.685 & 2.278 & 109.629 \\ % k=22
        \bottomrule
    \end{tabular}
    \caption{Performance report for Multivariate Regression}
    \label{tab:multi_regression_report}
\end{table}
The linear models had generally similar performances. Regularization seemed to provide some improvements
on some metrics, but overall the differences were not particularly significant, and both $\alpha$ parameters
were kept low by the Cross-Validation.
This suggests that the Linear model did not need particular regularization to perform at its best.
Similarly to the previous task, the linear models had worse performances than the Decision Tree and K-NN
regressors on the \texttt{criticReviewsTotal} target variable.
The performance on this variable shows that all models achieved lower R$^2$ scores compared to those for
\texttt{userReviewsTotal}, suggesting that its variance is more difficult to explain.\\
For \texttt{userReviewsTotal}, all linear models achieved higher R$^2$ scores than the Decision Tree and
K-Nearest Neighbors models, with the Lasso and Linear regressors performing best and yielding lower Mean Squared
Errors. However, their Mean Absolute Errors were higher than those of both the Decision Tree and K-NN regressors.
This suggests that the linear models were able to capture a substantial portion of the variance in the target variable,
but struggled to provide accurate predictions for individual titles. This may be due to the presence of outliers and
noise in the dataset, as indicated by the generally high Mean Squared Error across all models.
Additionally, the linear models may be limited by underlying non-linear relationships between features and the target variable.
This last hypothesis is further supported by the high complexity of the Decision Tree (maximum depth of 45) and the K-NN regressor
(which considered 22 neighbors), both of which adapted to more complex patterns in the data.
